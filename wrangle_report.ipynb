{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reporting: wragle_report\n",
    "* Create a **300-600 word written report** called \"wrangle_report.pdf\" or \"wrangle_report.html\" that briefly describes your wrangling efforts. This is to be framed as an internal document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Report: Details of Data Wrangling Project\n",
    "## Table of Contents\n",
    "<ul>\n",
    "<li><a href=\"#intro\">Introduction</a></li>\n",
    "<li><a href=\"#gathering\">Data Gathering</a></li>\n",
    "<li><a href=\"#assessing\">Data Assessment</a></li>\n",
    "<li><a href =\"#cleaning\">Data cleaning</a></li>   \n",
    "<li><a href=\"#conclusions\">Conclusions</a></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Data wrangling is the process of cleaning and unifying messy and complex data sets for easy access and analysis. With the amount of data and data sources rapidly growing and expanding, it is getting increasingly essential for large amounts of available data to be organized for analysis. That said, at the Udacity Data analyst Nanodegree program second project centred on data wrangling, i have written this report to effectively document the whole process*\n",
    "\n",
    "**data wrangling is divided into three phases namely:**\n",
    "- Data Gathering\n",
    "- Data assessing\n",
    "- Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The next set of information aims to explain the steps taken to wrangle the provided dataset*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='gathering'></a>\n",
    "## Data Gathering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*As directed by Udacity, The data for this project is to be gathered from three different sources namely:*\n",
    "- A csv file named `twitter_enhanced_archive`.\n",
    "- A url link to tsv file named `image_predictions.tsv`.\n",
    "- Twitter API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The csv file was provided by udacity and made available for download, after which i then read it into a dataframe using pandas `.read_csv` function.\n",
    "\n",
    "\n",
    "2. The `image_predictions.tsv` was downloaded using the url provided by Udacity. Using the requests library to get the contents of the provided url and then read the contents to a filepath = my present working directory. Afterwards, the tsv file is then read into a pandas dataframe.\n",
    "\n",
    "\n",
    "3. For getting data from the twitter API, i installed the twitter API library Tweepy and then using API tokens and keys gotten from Twitter elevated access to their API, i connected to the API.\n",
    "    thereafter, i used a for loop to loop through the twitter API using Tweet id from the enhanced archive and then dump the info into a json file using `json.dump()`.I then used the `.read_json()` method to read my file to a pandas dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='assessing'></a>\n",
    "## Data Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*In this section,i am to detect and document at least eight (8) quality issues and two (2) tidiness issue. Also i must use both visual assessment programmatic assessement to assess the data.*\n",
    "\n",
    "\n",
    "*Some conditions were stated prior to assessing the data. This conditions help to guide the quality and limit of the assessment.Below are some of the conditions to meet:*\n",
    "- You only want original ratings (no retweets) that have images\n",
    "- he requirements of this project are only to assess and clean at least 8 quality issues and at least 2 tidiness issues in this dataset\n",
    "- You do not need to gather the tweets beyond August 1st, 2017\n",
    "\n",
    "\n",
    "*I could go on and on with my assessment but conditions like that listed above help to save time and give direction to analysis.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Below are the issues detected in my dataset grouped into data quality and tidiness issues:*\n",
    "\n",
    "### Quality issues\n",
    "#### Twitter enhanced data(df_tae)\n",
    "1.incorrect datatypes of timestamp column\n",
    "\n",
    "2.missing names represented as None and incorrect names in the tail of the dataframe e.g a, quite e.t.c(seems names starting with small letters are incorrect)\n",
    "\n",
    "3.convert tweet_id to string datatype\n",
    "\n",
    "4.drop irrelevant records(retweeted status_id, in_reply_to_status_id e.t.c)\n",
    "\n",
    "5.incorrect values in the rating_denominator and numerator columns\n",
    "\n",
    "#### df_ip\n",
    "6.presnce of non_dog predictions in the predictions columns\n",
    "\n",
    "7.the need to capitalize dog breeds and remove '_'.\n",
    "\n",
    "8.duplicate jpg_url \n",
    "\n",
    "9.convert the tweet_id datatype to string\n",
    "\n",
    "#### df_tweet\n",
    "10.Rename id to tweet_id in df_tweet_clean for merge purposes\n",
    "\n",
    "\n",
    "\n",
    "### Tidiness issues\n",
    "#### Twitter enhanced data\n",
    "1. dog stages should be one column instead of four\n",
    "#### df_ip\n",
    "\n",
    "\n",
    "2. there should be only one master dataframe instead of three dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='cleaning'></a>\n",
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This involves transformation and working on the highlights of the data assessment stage.Explained below are the highlights of how the issues were solved;*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. *Remove irrelevant records(retweets and reply_to_status records) in df_tea: i simply filtered out the irrelevant data from my dataframe using array indexing*\n",
    "\n",
    "\n",
    "2. *incorrect values in the rating_denominator and numerator columns(df_tae_clean): here, i dropped the rating_denominator column and rename the rating_numerator column to rating_over_10 to accurately describe the characteristics of the column. I also made corrections to the incorrect values in the numerator column by first converting its datatype to float and then correct outliers by correctly extracting them from the text column and dropped ratings with extreme values and ratings that are not actually ratings.*\n",
    "\n",
    "3. *Remove duplicate jpg_url(df_ip): here, i removed duplicate image url of dogs in the df_ip dataframe. hence leaving one unique image for each dog for each tweet_id*\n",
    "\n",
    "4. *i also converted the neccessary incorrect datatypes to the suitable ones for analysis*\n",
    "\n",
    "5. *missing names represented as None and incorrect names in the tail of the dataframe(df_tae): Here, i will replace non_standard names(names that begin with small letter appear to be non-standard) with a regular standard like a null value*\n",
    "\n",
    "6. *presnce of non_dog predictions in the predictions columns(df_ip): To get the closest dog predictions, i removed all instances of false predictions that occured thrice for all dog predictions*\n",
    "\n",
    "7. *dog stages should be one column instead of four(df_tae_clean): Here, i want to have a dog_type column that will replace the four different columns previously used.*\n",
    "\n",
    "8. *There should be only one master dataframe instead of three dataframes: i simply merged the three dataframe together using the primary key `tweet_id`*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='conclusions'></a>\n",
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In onclusion, data wrangling is a key part of data analysis and should not be taken for granted. I cannot say my data is 100% clean as there are some more data quality issues in the dataset but it is clean enough to for my use-case. hence, i will be going forward in my analysis and drawing actionable insights from the data.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
